# Online Banking Analysis using Apache Spark

## Project Overview

This project focuses on analyzing large-scale online banking data using **Apache Spark** and its ecosystem. The primary goal is to demonstrate how big data tools can be used to ingest, process, and analyze high-volume financial datasets efficiently.

The datasets used in this project were sourced from **Kaggle** and include information related to **customers, loans, credit cards, and online transactions**. These datasets were cleaned, transformed, and analyzed to derive meaningful insights using distributed data processing techniques.

---

## Data Pipeline & Workflow

The project follows a structured big data workflow:

1. **Data Ingestion**

   * Downloaded datasets from Kaggle
   * Stored raw data in cloud storage
   * Imported structured data from **MySQL to Hive** using **Sqoop**

2. **Data Storage**

   * Used **HDFS** for distributed data storage
   * Managed structured data using **Hive tables**

3. **Data Processing & Analysis**

   * Processed large datasets using **HiveQL**
   * Performed advanced data analysis using **PySpark** in Jupyter Notebook
   * Implemented multiple analytical use cases on transactions, loans, and customer behavior

---

## Key Use Cases

* Customer transaction analysis
* Maximum and minimum withdrawal analysis
* Loan and credit card usage patterns
* Account-wise financial summaries
* Performance comparison of traditional SQL vs Spark SQL

---

## Technologies Used

* **Apache Spark**
* **Spark SQL**
* **PySpark**
* **HDFS**
* **Apache Hive**
* **MySQL**
* **Sqoop**
* **Git & GitHub**
* **Jupyter Notebook**

---

## Roles & Responsibilities

* Worked collaboratively in a team of **6 members** using Git/GitHub for version control
* Collected and explored multiple banking-related datasets from Kaggle
* Performed data cleaning and preprocessing
* Built Spark sessions and loaded data into Spark DataFrames
* Executed Spark SQL queries in a **standalone Spark cluster environment**
* Analyzed large-scale data efficiently using distributed computing concepts

---

## Outcome

This project strengthened practical understanding of:

* Big data architecture
* Distributed data processing
* Spark SQL and PySpark DataFrame operations
* End-to-end data engineering workflows

---

